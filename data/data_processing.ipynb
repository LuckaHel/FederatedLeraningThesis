{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5906c4cb-caab-49d8-9766-66b10921ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/hel/bigData/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /home/hel/bigData/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/hel/bigData/lib/python3.12/site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hel/bigData/lib/python3.12/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hel/bigData/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hel/bigData/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, split, array_join, expr, lower\n",
    "!pip install numpy\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "! pip install scikit-learn\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .appName(\"BERTDataPreprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "09fbadc1-eeb5-4e9b-97cd-2cacfe9b9cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20492509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 529:=====================================================> (48 + 1) / 49]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique English-language users in the Reddit comments dataset: 13616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#df_comments_MB = spark.read.csv(\"./mbti9k_comments.csv\")  # prob wont use this \n",
    "#df_post = spark.read.csv(\"./typed_posts.csv\") # this has numeric data which is not suitable for distilled bert, might try it with differenr model if time allows\n",
    "\n",
    "# Renaming columns in dataset mbti_1.csv\n",
    "df_mbti_1 = spark.read.csv(\"mbti_1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Renaming columns in dataset typed_comments.csv\n",
    "df_comments_typed = spark.read.csv(\"typed_comments.csv\", header=True, inferSchema=True)\n",
    "\n",
    "from pyspark.sql.functions import col, countDistinct, trim, lower\n",
    "\n",
    "# Step 1: Normalize 'lang' column to lowercase, trim spaces, and filter only English rows\n",
    "df_english = df_comments_typed.filter(lower(trim(col(\"lang\"))) == \"en\")\n",
    "print(df_english.count())\n",
    "\n",
    "# Step 3: Count unique users based on the 'author' column\n",
    "unique_users_count = df_english.select(countDistinct(\"author\")).collect()[0][0]\n",
    "\n",
    "# Display the number of unique users\n",
    "print(f\"Number of unique English-language users in the Reddit comments dataset: {unique_users_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c410dfda-c44d-4935-852f-2964a37ab784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('type', StringType(), True), StructField('comment', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Dropping irrelevant columns \n",
    "\n",
    "df_comments_typed = (\n",
    "    df_comments_typed\n",
    "    .filter(col(\"comment\").isNotNull() & (col(\"comment\") != \"\"))  # Non-empty comments\n",
    "    .filter(col(\"word_count\") > 5)  # Minimum word count\n",
    ")\n",
    "df_comments_typed_dropped = df_comments_typed[['type', 'comment']]\n",
    "\n",
    "print(df_comments_typed_dropped.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8a428bd5-0862-4e2a-aa14-59f4857d528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8675\n",
      "+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|type|post                                                                                                                                                                                                     |\n",
      "+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|INFJ|enfp and intj moments    sportscenter not top ten plays    pranks                                                                                                                                        |\n",
      "|INFJ|What has been the most life-changing experience in your life?                                                                                                                                            |\n",
      "|INFJ|On repeat for most of today.                                                                                                                                                                             |\n",
      "|INFJ|May the PerC Experience immerse you.                                                                                                                                                                     |\n",
      "|INFJ|The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~                                                                                              |\n",
      "|INFJ|Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as... |\n",
      "|INFJ|84389  84390     ...                                                                                                                                                                                     |\n",
      "|INFJ|Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...|\n",
      "|INFJ|Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...   |\n",
      "|INFJ|All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...      |\n",
      "|INFJ|Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:                                                                                   |\n",
      "|INFJ|It appears to be too late. :sad:                                                                                                                                                                         |\n",
      "|INFJ|There's someone out there for everyone.                                                                                                                                                                  |\n",
      "|INFJ|Wait... I thought confidence was a good thing.                                                                                                                                                           |\n",
      "|INFJ|I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...|\n",
      "|INFJ|Yo entp ladies... if you're into a complimentary personality,well, hey.                                                                                                                                  |\n",
      "|INFJ|... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.                                                                                                  |\n",
      "|INFJ|I really dig the part from 1:46 to 2:50                                                                                                                                                                  |\n",
      "|INFJ|Banned because this thread requires it of me.                                                                                                                                                            |\n",
      "|INFJ|Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.                                                              |\n",
      "+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 539:>                                                      (0 + 15) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, regexp_replace, trim, size, col\n",
    "\n",
    "# Step 1: Normalize multiple delimiters\n",
    "df_cleaned = df_mbti_1.withColumn(\"posts\", regexp_replace(df_mbti_1[\"posts\"], r\"(\\|\\|\\|)+\", \"|||\"))\n",
    "print(df_cleaned.count())\n",
    "# Step 2: Split posts into an array\n",
    "df_split = df_cleaned.withColumn(\"post\", split(df_cleaned[\"posts\"], r\"\\|\\|\\|\"))\n",
    "\n",
    "# Step 3: Explode the array to create separate rows\n",
    "df_exploded = df_split.select(\"type\", explode(df_split[\"post\"]).alias(\"post\"))\n",
    "\n",
    "# Step 4: Remove URLs using regex\n",
    "df_no_links = df_exploded.withColumn(\"post\", regexp_replace(col(\"post\"), r\"http\\S+|www\\S+|\\S+\\.(com|net|org|io|gov|edu)\\S*\", \"\"))\n",
    "\n",
    "# Step 5: Trim whitespace and remove empty rows\n",
    "df_trimmed = df_no_links.withColumn(\"post\", trim(col(\"post\"))) \\\n",
    "                        .filter(col(\"post\") != \"\")\n",
    "\n",
    "# Step 6: Remove posts with fewer than 5 words\n",
    "df_mbti_1= df_trimmed.filter(size(split(col(\"post\"), \" \")) >= 5)\n",
    "\n",
    "# Step 7: Show cleaned dataset\n",
    "df_mbti_1.show(truncate=False)\n",
    "print(df_mbti_1.count())\n",
    "\n",
    "# Optional: Save the cleaned dataset\n",
    "# df_filtered.write.csv(\"mbti_1_filtered.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f1e3d655-6ec6-4b70-b536-99bb93a89782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments_types schema StructType([StructField('type', StringType(), True), StructField('post', StringType(), True)])\n",
      "MBTI schema StructType([StructField('type', StringType(), True), StructField('post', StringType(), False)])\n"
     ]
    }
   ],
   "source": [
    "# Rename 'comment' to 'posts'\n",
    "df_comments_typed_dropped = df_comments_typed_dropped.withColumnRenamed('comment', 'post')\n",
    "\n",
    "\n",
    "\n",
    "# Convert all values in the 'type' column to lowercase\n",
    "from pyspark.sql.functions import lower, col\n",
    "df_comments_typed_dropped = df_comments_typed_dropped.withColumn(\"type\", lower(col(\"type\")))\n",
    "df_mbti_1 = df_mbti_1.withColumn(\"type\", lower(col(\"type\")))\n",
    "\n",
    "#filter out coment less < 5 words \n",
    "df_comments_typed_dropped= df_comments_typed_dropped.filter(size(split(col(\"post\"), \" \")) >= 5)\n",
    "\n",
    "\n",
    "# Ensure the schema is exactly the same on both dataframes before joining them\n",
    "print(\"Comments_types schema\", df_comments_typed_dropped.schema)\n",
    "print(\"MBTI schema\", df_mbti_1.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8093dbd2-7e06-4dbb-b0be-ae726d32c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments_types row count 12691384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBTI row count 383864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|type|                post|\n",
      "+----+--------------------+\n",
      "|infj|enfp and intj mom...|\n",
      "|infj|What has been the...|\n",
      "|infj|On repeat for mos...|\n",
      "|infj|May the PerC Expe...|\n",
      "|infj|The last thing my...|\n",
      "|entp|Those stats come ...|\n",
      "|entp|\"That's great to ...|\n",
      "|entp|I can totally agr...|\n",
      "|entp|\"I took it severa...|\n",
      "|entp|Gawd it's like we...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 553:============================================>         (53 + 11) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union dataframe row count 13075248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Join dataframes\n",
    "print(\"Comments_types row count\", df_comments_typed_dropped.count())\n",
    "print(\"MBTI row count\", df_mbti_1.count())\n",
    "\n",
    "#df_mbti_1.sort(asc(\"type\")).limit(10).show()\n",
    "#df_comments_typed_dropped.limit(10).show()\n",
    "\n",
    "df_short_mbti = df_mbti_1.limit(5)\n",
    "df_short_comments = df_comments_typed_dropped.limit(5)\n",
    "\n",
    "df_short_union = df_short_mbti.union(df_short_comments)\n",
    "\n",
    "df_short_union.show()\n",
    "\n",
    "df_union = df_mbti_1.union(df_comments_typed_dropped)\n",
    "\n",
    "print(\"Union dataframe row count\", df_union.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06b8e5af-468a-4387-8899-20705baf1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df_union = df_union.sort(asc(\"type\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35aebe26-36cc-452e-af0c-96bb95d6a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 682:======================================================>(63 + 1) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV files successfully created with the following row counts and percentages:\n",
      "estp: 176051 rows (1.35%)\n",
      "entj: 1244567 rows (9.52%)\n",
      "estj: 185015 rows (1.42%)\n",
      "enfp: 461949 rows (3.53%)\n",
      "istj: 769810 rows (5.89%)\n",
      "enfj: 373265 rows (2.85%)\n",
      "isfj: 453292 rows (3.47%)\n",
      "istp: 775190 rows (5.93%)\n",
      "intp: 2910987 rows (22.26%)\n",
      "infj: 830725 rows (6.35%)\n",
      "intj: 2258382 rows (17.27%)\n",
      "isfp: 323723 rows (2.48%)\n",
      "entp: 813315 rows (6.22%)\n",
      "infp: 908018 rows (6.94%)\n",
      "esfp: 204347 rows (1.56%)\n",
      "esfj: 386612 rows (2.96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define output directory for CSV files\n",
    "output_dir = \"mbti_split_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Get unique personality types\n",
    "personality_types = [row[0] for row in df_union.select(\"type\").distinct().collect()]\n",
    "\n",
    "# Get total number of rows in the dataset\n",
    "total_rows = df_union.count()\n",
    "\n",
    "# Dictionary to store row counts and percentages\n",
    "row_counts = {}\n",
    "\n",
    "# Iterate over personality types, save CSVs, and count rows\n",
    "for p_type in personality_types:\n",
    "    # Filter DataFrame for the current personality type\n",
    "    df_filtered = df_union.filter(df_union[\"type\"] == p_type)\n",
    "\n",
    "    # Save the filtered DataFrame as a CSV file with overwrite mode\n",
    "    file_path = f\"{output_dir}/{p_type}.csv\"\n",
    "    df_filtered.write.mode(\"overwrite\").csv(file_path, header=True)\n",
    "\n",
    "    # Count the number of rows in the filtered DataFrame\n",
    "    count = df_filtered.count()\n",
    "    \n",
    "    # Calculate percentage of total dataset\n",
    "    percentage = (count / total_rows) * 100\n",
    "    \n",
    "    # Store the count and percentage\n",
    "    row_counts[p_type] = (count, round(percentage, 2))\n",
    "\n",
    "# Print row counts and percentages\n",
    "print(\"✅ CSV files successfully created with the following row counts and percentages:\")\n",
    "for p_type, (count, percentage) in row_counts.items():\n",
    "    print(f\"{p_type}: {count} rows ({percentage}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e379533-a5e8-4621-a9ca-dfade82612a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Drop \"type_indexed\" and \"type_encoded\" columns if they already exist\n",
    "for col_name in [\"type_indexed\", \"type_encoded\"]:\n",
    "    if col_name in df_comments_typed.columns:\n",
    "        df_comments_typed = df_comments_typed.drop(col_name)\n",
    "\n",
    "# Apply StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_indexed\", handleInvalid=\"keep\")\n",
    "df_comments_typed = indexer.fit(df_comments_typed).transform(df_comments_typed)\n",
    "\n",
    "# Apply OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCol=\"type_indexed\", outputCol=\"type_encoded\")\n",
    "df_comments_typed = encoder.fit(df_comments_typed).transform(df_comments_typed)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "df_pandas = df_comments_typed.select(\"comment\", \"type_encoded\").limit(5000).toPandas()\n",
    "df_pandas[\"type_encoded\"] = df_pandas[\"type_encoded\"].apply(lambda x: x.toArray())\n",
    "df_pandas[\"class_label\"] = df_pandas[\"type_encoded\"].apply(lambda x: x.argmax())\n",
    "\n",
    "# Train-Test Split\n",
    "train_df, test_df = train_test_split(df_pandas, test_size=0.1, stratify=df_pandas[\"class_label\"], random_state=42)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize_text(df): \n",
    "    return tokenizer(df[\"comment\"].tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "train_encodings, test_encodings = map(tokenize_text, [train_df, test_df])\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Convert Pandas arrays to proper NumPy float32 before creating Torch tensors\n",
    "train_labels, test_labels = map(\n",
    "    lambda df: torch.tensor(np.stack(df[\"type_encoded\"].values).astype(np.float32)), \n",
    "    [train_df, test_df]\n",
    ")\n",
    "\n",
    "# Save datasets\n",
    "for name, enc, lbl in [(\"train\", train_encodings, train_labels), (\"test\", test_encodings, test_labels)]:\n",
    "    torch.save({\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": lbl}, f\"{name}_data.pth\", _use_new_zipfile_serialization=False)\n",
    "\n",
    "print(\"✅ Datasets saved as train_data.pth & test_data.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e5bc2-618b-4308-860f-111825a8e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introvert extrivert separation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a96b40-a1fe-48d5-8c3a-df68f5a24ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
