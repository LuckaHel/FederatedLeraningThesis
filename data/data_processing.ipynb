{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5906c4cb-caab-49d8-9766-66b10921ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/hel/bigData/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /home/hel/bigData/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/hel/bigData/lib/python3.12/site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hel/bigData/lib/python3.12/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hel/bigData/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hel/bigData/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 11:02:27 WARN Utils: Your hostname, hel-IdeaPad-Slim-5-14AHP9 resolves to a loopback address: 127.0.1.1; using 10.10.95.219 instead (on interface wlp2s0)\n",
      "25/03/13 11:02:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 11:02:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/13 11:02:43 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, split, array_join, expr, lower\n",
    "!pip install numpy\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "! pip install scikit-learn\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .appName(\"BERTDataPreprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fbadc1-eeb5-4e9b-97cd-2cacfe9b9cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20492509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:======================================================>  (47 + 2) / 49]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique English-language users in the Reddit comments dataset: 13616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#df_comments_MB = spark.read.csv(\"./mbti9k_comments.csv\")  # prob wont use this \n",
    "#df_post = spark.read.csv(\"./typed_posts.csv\") # this has numeric data which is not suitable for distilled bert, might try it with differenr model if time allows\n",
    "\n",
    "# Renaming columns in dataset mbti_1.csv\n",
    "df_mbti_1 = spark.read.csv(\"mbti_1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Renaming columns in dataset typed_comments.csv\n",
    "df_comments_typed = spark.read.csv(\"typed_comments.csv\", header=True, inferSchema=True)\n",
    "\n",
    "from pyspark.sql.functions import col, countDistinct, trim, lower\n",
    "\n",
    "# Step 1: Normalize 'lang' column to lowercase, trim spaces, and filter only English rows\n",
    "df_english = df_comments_typed.filter(lower(trim(col(\"lang\"))) == \"en\")\n",
    "print(df_english.count())\n",
    "\n",
    "# Step 3: Count unique users based on the 'author' column\n",
    "unique_users_count = df_english.select(countDistinct(\"author\")).collect()[0][0]\n",
    "\n",
    "# Display the number of unique users\n",
    "print(f\"Number of unique English-language users in the Reddit comments dataset: {unique_users_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8228152-e1ec-47a4-abf4-e3220c9c6467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c410dfda-c44d-4935-852f-2964a37ab784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('type', StringType(), True), StructField('comment', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Dropping irrelevant columns \n",
    "\n",
    "df_comments_typed = (\n",
    "    df_comments_typed\n",
    "    .filter(col(\"comment\").isNotNull() & (col(\"comment\") != \"\"))  # Non-empty comments\n",
    "    .filter(col(\"word_count\") > 5)  # Minimum word count\n",
    ")\n",
    "df_comments_typed_dropped = df_comments_typed[['type', 'comment']]\n",
    "\n",
    "print(df_comments_typed_dropped.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a428bd5-0862-4e2a-aa14-59f4857d528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8675\n",
      "+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|type|post                                                                                                                                                                                                     |\n",
      "+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|INFJ|enfp and intj moments    sportscenter not top ten plays    pranks                                                                                                                                        |\n",
      "|INFJ|What has been the most life-changing experience in your life?                                                                                                                                            |\n",
      "|INFJ|On repeat for most of today.                                                                                                                                                                             |\n",
      "|INFJ|May the PerC Experience immerse you.                                                                                                                                                                     |\n",
      "|INFJ|The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~                                                                                              |\n",
      "|INFJ|Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as... |\n",
      "|INFJ|84389  84390     ...                                                                                                                                                                                     |\n",
      "|INFJ|Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...|\n",
      "|INFJ|Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...   |\n",
      "|INFJ|All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...      |\n",
      "|INFJ|Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:                                                                                   |\n",
      "|INFJ|It appears to be too late. :sad:                                                                                                                                                                         |\n",
      "|INFJ|There's someone out there for everyone.                                                                                                                                                                  |\n",
      "|INFJ|Wait... I thought confidence was a good thing.                                                                                                                                                           |\n",
      "|INFJ|I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...|\n",
      "|INFJ|Yo entp ladies... if you're into a complimentary personality,well, hey.                                                                                                                                  |\n",
      "|INFJ|... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.                                                                                                  |\n",
      "|INFJ|I really dig the part from 1:46 to 2:50                                                                                                                                                                  |\n",
      "|INFJ|Banned because this thread requires it of me.                                                                                                                                                            |\n",
      "|INFJ|Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.                                                              |\n",
      "+----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                       (0 + 15) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, regexp_replace, trim, size, col\n",
    "\n",
    "# Step 1: Normalize multiple delimiters\n",
    "df_cleaned = df_mbti_1.withColumn(\"posts\", regexp_replace(df_mbti_1[\"posts\"], r\"(\\|\\|\\|)+\", \"|||\"))\n",
    "print(df_cleaned.count())\n",
    "# Step 2: Split posts into an array\n",
    "df_split = df_cleaned.withColumn(\"post\", split(df_cleaned[\"posts\"], r\"\\|\\|\\|\"))\n",
    "\n",
    "# Step 3: Explode the array to create separate rows\n",
    "df_exploded = df_split.select(\"type\", explode(df_split[\"post\"]).alias(\"post\"))\n",
    "\n",
    "# Step 4: Remove URLs using regex\n",
    "df_no_links = df_exploded.withColumn(\"post\", regexp_replace(col(\"post\"), r\"http\\S+|www\\S+|\\S+\\.(com|net|org|io|gov|edu)\\S*\", \"\"))\n",
    "\n",
    "# Step 5: Trim whitespace and remove empty rows\n",
    "df_trimmed = df_no_links.withColumn(\"post\", trim(col(\"post\"))) \\\n",
    "                        .filter(col(\"post\") != \"\")\n",
    "\n",
    "# Step 6: Remove posts with fewer than 5 words\n",
    "df_mbti_1= df_trimmed.filter(size(split(col(\"post\"), \" \")) >= 5)\n",
    "\n",
    "# Step 7: Show cleaned dataset\n",
    "df_mbti_1.show(truncate=False)\n",
    "print(df_mbti_1.count())\n",
    "\n",
    "# Optional: Save the cleaned dataset\n",
    "# df_filtered.write.csv(\"mbti_1_filtered.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e3d655-6ec6-4b70-b536-99bb93a89782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments_types schema StructType([StructField('type', StringType(), True), StructField('post', StringType(), True)])\n",
      "MBTI schema StructType([StructField('type', StringType(), True), StructField('post', StringType(), False)])\n"
     ]
    }
   ],
   "source": [
    "# Rename 'comment' to 'posts'\n",
    "df_comments_typed_dropped = df_comments_typed_dropped.withColumnRenamed('comment', 'post')\n",
    "\n",
    "\n",
    "\n",
    "# Convert all values in the 'type' column to lowercase\n",
    "from pyspark.sql.functions import lower, col\n",
    "df_comments_typed_dropped = df_comments_typed_dropped.withColumn(\"type\", lower(col(\"type\")))\n",
    "df_mbti_1 = df_mbti_1.withColumn(\"type\", lower(col(\"type\")))\n",
    "\n",
    "#filter out coment less < 5 words \n",
    "df_comments_typed_dropped= df_comments_typed_dropped.filter(size(split(col(\"post\"), \" \")) >= 5)\n",
    "\n",
    "\n",
    "# Ensure the schema is exactly the same on both dataframes before joining them\n",
    "print(\"Comments_types schema\", df_comments_typed_dropped.schema)\n",
    "print(\"MBTI schema\", df_mbti_1.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8093dbd2-7e06-4dbb-b0be-ae726d32c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments_types row count 12691384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBTI row count 383864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|type|                post|\n",
      "+----+--------------------+\n",
      "|infj|enfp and intj mom...|\n",
      "|infj|What has been the...|\n",
      "|infj|On repeat for mos...|\n",
      "|infj|May the PerC Expe...|\n",
      "|infj|The last thing my...|\n",
      "|entp|Those stats come ...|\n",
      "|entp|\"That's great to ...|\n",
      "|entp|I can totally agr...|\n",
      "|entp|\"I took it severa...|\n",
      "|entp|Gawd it's like we...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=============================================>         (53 + 11) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union dataframe row count 13075248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Join dataframes\n",
    "print(\"Comments_types row count\", df_comments_typed_dropped.count())\n",
    "print(\"MBTI row count\", df_mbti_1.count())\n",
    "\n",
    "#df_mbti_1.sort(asc(\"type\")).limit(10).show()\n",
    "#df_comments_typed_dropped.limit(10).show()\n",
    "\n",
    "df_short_mbti = df_mbti_1.limit(5)\n",
    "df_short_comments = df_comments_typed_dropped.limit(5)\n",
    "\n",
    "df_short_union = df_short_mbti.union(df_short_comments)\n",
    "\n",
    "df_short_union.show()\n",
    "\n",
    "df_union = df_mbti_1.union(df_comments_typed_dropped)\n",
    "\n",
    "print(\"Union dataframe row count\", df_union.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b8e5af-468a-4387-8899-20705baf1b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7c8964fcc4a0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df_union = df_union.sort(asc(\"type\"))\n",
    "print(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35aebe26-36cc-452e-af0c-96bb95d6a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 160:======================================================>(63 + 1) / 64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV files successfully created with the following row counts and percentages:\n",
      "estp: 176051 rows (1.35%)\n",
      "entj: 1244567 rows (9.52%)\n",
      "estj: 185015 rows (1.42%)\n",
      "enfp: 461949 rows (3.53%)\n",
      "istj: 769810 rows (5.89%)\n",
      "enfj: 373265 rows (2.85%)\n",
      "isfj: 453292 rows (3.47%)\n",
      "istp: 775190 rows (5.93%)\n",
      "intp: 2910987 rows (22.26%)\n",
      "infj: 830725 rows (6.35%)\n",
      "intj: 2258382 rows (17.27%)\n",
      "isfp: 323723 rows (2.48%)\n",
      "entp: 813315 rows (6.22%)\n",
      "infp: 908018 rows (6.94%)\n",
      "esfp: 204347 rows (1.56%)\n",
      "esfj: 386612 rows (2.96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define output directory for CSV files\n",
    "output_dir = \"mbti_split_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Get unique personality types\n",
    "personality_types = [row[0] for row in df_union.select(\"type\").distinct().collect()]\n",
    "\n",
    "# Get total number of rows in the dataset\n",
    "\n",
    "\n",
    "total_rows = df_union.count()\n",
    "\n",
    "# Dictionary to store row counts and percentages\n",
    "row_counts = {}\n",
    "\n",
    "# Iterate over personality types, save CSVs, and count rows\n",
    "for p_type in personality_types:\n",
    "    # Filter DataFrame for the current personality type\n",
    "    df_filtered = df_union.filter(df_union[\"type\"] == p_type)\n",
    "\n",
    "    # Save as a single CSV file in a temporary directory\n",
    "    temp_dir = f\"{output_dir}/{p_type}\"\n",
    "    df_filtered.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_dir)\n",
    "\n",
    "    # Find the actual CSV file inside the created directory\n",
    "    for file in os.listdir(temp_dir):\n",
    "        if file.startswith(\"part-\") and file.endswith(\".csv\"):\n",
    "            os.rename(os.path.join(temp_dir, file), os.path.join(output_dir, f\"{p_type}.csv\"))\n",
    "            break  # Stop once we rename the first file\n",
    "\n",
    "    # Remove the now-empty directory Spark created\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "    # Count the number of rows in the filtered DataFrame\n",
    "    count = df_filtered.count()\n",
    "\n",
    "    # Calculate percentage of total dataset\n",
    "    percentage = (count / total_rows) * 100\n",
    "\n",
    "    # Store the count and percentage\n",
    "    row_counts[p_type] = (count, round(percentage, 2))\n",
    "\n",
    "# Print row counts and percentages\n",
    "print(\"✅ CSV files successfully created with the following row counts and percentages:\")\n",
    "for p_type, (count, percentage) in row_counts.items():\n",
    "    print(f\"{p_type}: {count} rows ({percentage}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63eae580-ec81-46b2-8995-9bbdad44b2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|type|                post|\n",
      "+----+--------------------+\n",
      "|entj|Um, Hilter for su...|\n",
      "|entj|Yes and also when...|\n",
      "|entj|I am done, done, ...|\n",
      "|entj|He strikes me as ...|\n",
      "|entj|I haven't dealt w...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import typing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from random import randint\n",
    "\n",
    "\n",
    "# Define paths and constants\n",
    "PATH_TO_CSVS = \"./mbti_split_data\"\n",
    "NUMBER_OF_PERSONALITIES = len(os.listdir(PATH_TO_CSVS))\n",
    "NUMBER_OF_CLIENTS = 10\n",
    "SAMPLE_SIZE_PER_PERSONALITY = 15000\n",
    "SAMPLE_SIZE_PER_CLIENT = SAMPLE_SIZE_PER_PERSONALITY * NUMBER_OF_PERSONALITIES\n",
    "\n",
    "def create_samples_for_all_clients() -> list:\n",
    "\n",
    "    samples = []\n",
    "    for _ in range(NUMBER_OF_CLIENTS):\n",
    "        samples.append(create_sample_for_one_client())\n",
    "\n",
    "    return samples\n",
    "\n",
    "def create_sample_for_one_client():\n",
    "\n",
    "    client_sample = None  # Initialize empty Spark DataFrame\n",
    "\n",
    "    for personality_csv in os.listdir(PATH_TO_CSVS):\n",
    "        file_path = os.path.join(PATH_TO_CSVS, personality_csv)\n",
    "\n",
    "        try:\n",
    "            # Load CSV using Spark\n",
    "            personality_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"File {personality_csv} caused error '{e}' when loading.\")\n",
    "            continue\n",
    "        \n",
    "        # Determine if we need replacement\n",
    "        replace = SAMPLE_SIZE_PER_PERSONALITY > personality_df.count()\n",
    "\n",
    "        # Sample the DataFrame (Spark sampling equivalent)\n",
    "        sampled_df = personality_df.sample(withReplacement=replace, fraction=1.0 * SAMPLE_SIZE_PER_PERSONALITY / personality_df.count())\n",
    "\n",
    "        # Append to client_sample\n",
    "        if client_sample is None:\n",
    "            client_sample = sampled_df\n",
    "        else:\n",
    "            client_sample = client_sample.union(sampled_df)\n",
    "\n",
    "    return client_sample\n",
    "\n",
    "# Run and display client samples\n",
    "samples = create_samples_for_all_clients()\n",
    "\n",
    "# Show a sample from the first client dataset\n",
    "samples[0].show(5)  # Display first 5 rows from first client's dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e379533-a5e8-4621-a9ca-dfade82612a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas  as pd\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"MBTIProcessing\").getOrCreate()\n",
    "\n",
    "def convert_dfs_to_pth(df_list, output_dir=None, output_prefix=\"data\", test_fraction=0.1):\n",
    "\n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = os.getcwd()  # Default: same directory as notebook/script\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    # Determine number of test datasets (ensure at least 1)\n",
    "    num_test = max(1, int(len(df_list) * test_fraction))  \n",
    "    test_dfs = df_list[:num_test]   # First N datasets for test\n",
    "    train_dfs = df_list[num_test:]  # Remaining datasets for training\n",
    "\n",
    "    # ✅ Combine all test DataFrames using PySpark's `union()`\n",
    "    if len(test_dfs) > 1:\n",
    "        test_df = reduce(DataFrame.union, test_dfs)  # Merge multiple Spark DataFrames\n",
    "    else:\n",
    "        test_df = test_dfs[0]  # If only one DataFrame, use it directly\n",
    "\n",
    "    # Convert to Pandas for further processing\n",
    "    test_pandas = test_df.toPandas()\n",
    "\n",
    "    # ✅ Drop missing values and ensure 'post' column is a string\n",
    "    test_pandas = test_pandas.dropna(subset=[\"post\"])  # Remove NaN values\n",
    "    test_pandas[\"post\"] = test_pandas[\"post\"].astype(str)  # Convert all values to strings\n",
    "\n",
    "    # ✅ Convert 'type' column to numerical labels and ensure integer dtype\n",
    "    test_pandas[\"type_indexed\"], _ = pd.factorize(test_pandas[\"type\"])\n",
    "    test_pandas[\"type_indexed\"] = test_pandas[\"type_indexed\"].astype(int)  # Ensure integer labels\n",
    "\n",
    "    # ✅ One-hot encoding (fixed)\n",
    "    num_classes = test_pandas[\"type_indexed\"].nunique()\n",
    "    test_pandas[\"type_encoded\"] = test_pandas[\"type_indexed\"].apply(\n",
    "        lambda x: np.eye(num_classes, dtype=np.float32)[int(x)]\n",
    "    )\n",
    "\n",
    "    # Tokenize the global test set\n",
    "    test_encodings = tokenizer(\n",
    "        test_pandas[\"post\"].tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\"\n",
    "    )\n",
    "    test_labels = torch.tensor(np.stack(test_pandas[\"type_encoded\"].values).astype(np.float32))\n",
    "\n",
    "    # Save global test dataset\n",
    "    test_path = os.path.join(output_dir, f\"{output_prefix}_test.pth\")\n",
    "    torch.save({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"], \"labels\": test_labels}, test_path)\n",
    "    print(f\"✅ Global test dataset saved as {test_path}\")\n",
    "\n",
    "    saved_paths = {\"test\": test_path}\n",
    "\n",
    "    # Process each training dataset separately\n",
    "    for i, train_df in enumerate(train_dfs, start=1):\n",
    "        # Convert to Pandas (only if needed for tokenization)\n",
    "        train_pandas = train_df.toPandas()\n",
    "\n",
    "        # ✅ Drop missing values and ensure 'post' column is a string\n",
    "        train_pandas = train_pandas.dropna(subset=[\"post\"])  # Remove NaN values\n",
    "        train_pandas[\"post\"] = train_pandas[\"post\"].astype(str)  # Convert all values to strings\n",
    "\n",
    "        # ✅ Convert 'type' column to numerical labels and ensure integer dtype\n",
    "        train_pandas[\"type_indexed\"], _ = pd.factorize(train_pandas[\"type\"])\n",
    "        train_pandas[\"type_indexed\"] = train_pandas[\"type_indexed\"].astype(int)\n",
    "\n",
    "        # ✅ One-hot encoding (fixed)\n",
    "        num_classes = train_pandas[\"type_indexed\"].nunique()\n",
    "        train_pandas[\"type_encoded\"] = train_pandas[\"type_indexed\"].apply(\n",
    "            lambda x: np.eye(num_classes, dtype=np.float32)[int(x)]\n",
    "        )\n",
    "\n",
    "        # Tokenize train dataset\n",
    "        train_encodings = tokenizer(\n",
    "            train_pandas[\"post\"].tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\"\n",
    "        )\n",
    "        train_labels = torch.tensor(np.stack(train_pandas[\"type_encoded\"].values).astype(np.float32))\n",
    "\n",
    "        # Save train dataset\n",
    "        train_path = os.path.join(output_dir, f\"{output_prefix}{i}_train.pth\")\n",
    "        torch.save({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"labels\": train_labels}, train_path)\n",
    "        \n",
    "        saved_paths[f\"train_client_{i}\"] = train_path\n",
    "        print(f\"✅ Client {i} training dataset saved as {train_path}\")\n",
    "\n",
    "    return saved_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "422e5bc2-618b-4308-860f-111825a8e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Global test dataset saved as /home/hel/ThesisFL/data/data_test.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client 1 training dataset saved as /home/hel/ThesisFL/data/data1_train.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client 2 training dataset saved as /home/hel/ThesisFL/data/data2_train.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client 3 training dataset saved as /home/hel/ThesisFL/data/data3_train.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client 4 training dataset saved as /home/hel/ThesisFL/data/data4_train.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client 5 training dataset saved as /home/hel/ThesisFL/data/data5_train.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client 6 training dataset saved as /home/hel/ThesisFL/data/data6_train.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client 7 training dataset saved as /home/hel/ThesisFL/data/data7_train.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client 8 training dataset saved as /home/hel/ThesisFL/data/data8_train.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client 9 training dataset saved as /home/hel/ThesisFL/data/data9_train.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test': '/home/hel/ThesisFL/data/data_test.pth',\n",
       " 'train_client_1': '/home/hel/ThesisFL/data/data1_train.pth',\n",
       " 'train_client_2': '/home/hel/ThesisFL/data/data2_train.pth',\n",
       " 'train_client_3': '/home/hel/ThesisFL/data/data3_train.pth',\n",
       " 'train_client_4': '/home/hel/ThesisFL/data/data4_train.pth',\n",
       " 'train_client_5': '/home/hel/ThesisFL/data/data5_train.pth',\n",
       " 'train_client_6': '/home/hel/ThesisFL/data/data6_train.pth',\n",
       " 'train_client_7': '/home/hel/ThesisFL/data/data7_train.pth',\n",
       " 'train_client_8': '/home/hel/ThesisFL/data/data8_train.pth',\n",
       " 'train_client_9': '/home/hel/ThesisFL/data/data9_train.pth'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_dfs_to_pth(samples,output_dir=None, output_prefix=\"data\", test_fraction=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a96b40-a1fe-48d5-8c3a-df68f5a24ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/hel/ThesisFL/data/estp1.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestp1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df_spark \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Count total samples\u001b[39;00m\n\u001b[1;32m      5\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m df_spark\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m~/bigData/lib/python3.12/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/bigData/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/bigData/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/hel/ThesisFL/data/estp1.csv."
     ]
    }
   ],
   "source": [
    "\n",
    "csv_file = \"estp1.csv\"\n",
    "df_spark = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "\n",
    "# Count total samples\n",
    "total_samples = df_spark.count()\n",
    "print(f\"Total samples in dataset: {total_samples}\")\n",
    "\n",
    "# Define batch size (same as what you use in training)\n",
    "batch_size = 16  # Adjust based on your training setup\n",
    "\n",
    "# Calculate number of batches\n",
    "num_batches = math.ceil(total_samples / batch_size)\n",
    "print(f\"Total batches: {num_batches}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da0dd0-79b6-460d-8655-bdd9fb5a68ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
