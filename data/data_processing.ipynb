{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5906c4cb-caab-49d8-9766-66b10921ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./lib/python3.12/site-packages (2.2.2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, split, array_join, expr, lower\n",
    "!pip install numpy\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "! pip install scikit-learn\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .appName(\"BERTDataPreprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09fbadc1-eeb5-4e9b-97cd-2cacfe9b9cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 207:====================================================>  (47 + 2) / 49]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 22934562 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#df_comments_MB = spark.read.csv(\"./mbti9k_comments.csv\")  # prob wont use this \n",
    "#df_post = spark.read.csv(\"./typed_posts.csv\") # this has numeric data which is not suitable for distilled bert, might try it with differenr model if time allows\n",
    "\n",
    "# Renaming columns in dataset mbti_1.csv\n",
    "df_mbti_1 = spark.read.csv(\"mbti_1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Renaming columns in dataset typed_comments.csv\n",
    "df_comments_typed = spark.read.csv(\"typed_comments.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "print(f\"The DataFrame has {df_comments_typed.count()} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a419c0f0-ccb4-4bda-a4c2-c7e98f989c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|type|       post_combined|\n",
      "+----+--------------------+\n",
      "|infj|Prozac, wellbruti...|\n",
      "|entp|Well charlie I wi...|\n",
      "|intp|So-called Ti-Si l...|\n",
      "|intj|Very seriously to...|\n",
      "|entj|I probably would ...|\n",
      "|intj|There are as many...|\n",
      "|infj|I think this suck...|\n",
      "|intj|sometimes i look ...|\n",
      "|infj|Help or a voice o...|\n",
      "|intp|definitely Walter...|\n",
      "|infj|The dirty people ...|\n",
      "|enfj|I went through a ...|\n",
      "|infj|As an Fe user, I ...|\n",
      "|intj|'Fair enough, if ...|\n",
      "|intp|I enjoy that all ...|\n",
      "|intp|I saw that earlie...|\n",
      "|infj|My senses are all...|\n",
      "|infp|I appreciated tha...|\n",
      "|infj|I empathize with ...|\n",
      "|infp|At this, Job got ...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# UDF to clean and filter posts\n",
    "def clean_and_filter_posts(post):\n",
    "    posts = post.split('|||')\n",
    "    cleaned_posts = [\n",
    "        ' '.join([word for word in p.split() if not word.startswith('http')])  # Remove links\n",
    "        for p in posts if len(p.split()) >= 5  # Keep posts with 5 or more words\n",
    "    ]\n",
    "    return '|||'.join(cleaned_posts)\n",
    "\n",
    "# Apply the cleaning UDF to the post column in df_mbti_1\n",
    "df_cleaned = df_mbti_1.withColumn(\"posts\", udf(clean_and_filter_posts, StringType())(col(\"posts\")))\n",
    "\n",
    "\n",
    "# UDF to combine posts within a 512-token limit (~2560 words)\n",
    "def combine_posts(post):\n",
    "    posts = post.split('|||')\n",
    "    posts = sorted(posts, key=len, reverse=True)\n",
    "    max_length = 2560  # Approximate word limit\n",
    "    combined = \"\"\n",
    "    \n",
    "    for p in posts:\n",
    "        if len(combined.split()) + len(p.split()) <= max_length:\n",
    "            combined += \" \" + p\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return combined.strip()\n",
    "\n",
    "# Apply the UDF to combine cleaned posts in df_cleaned\n",
    "df_combined = df_cleaned.withColumn(\"post_combined\", udf(combine_posts, StringType())(col(\"posts\")))\n",
    "\n",
    "# Keep only the necessary columns for training\n",
    "df_final = df_combined.select(\"type\", \"post_combined\")\n",
    "df_final = df_final.withColumn('type', lower(df_final['type']))\n",
    "\n",
    "\n",
    "df_final.write.csv(\"processed_mbti_dataset.csv\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef30bdeb-b8cd-4f19-b9f9-0c4265095f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 218:===================================================>   (46 + 3) / 49]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11773174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# Valid MBTI types (lowercase)\n",
    "valid_types = [\"intj\", \"intp\", \"entj\", \"entp\", \"infj\", \"infp\", \"enfj\", \"enfp\",\n",
    "               \"istj\", \"isfj\", \"estj\", \"esfj\", \"istp\", \"isfp\", \"estp\", \"esfp\"]\n",
    "\n",
    "\n",
    "# Filter rows\n",
    "df_comments_typed = (\n",
    "    df_comments_typedc\n",
    "    .filter(col(\"comment\").isNotNull() & (col(\"comment\") != \"\"))  # Non-empty comments\n",
    "    .filter(lower(col(\"type\")).isin(valid_types))  # Valid MBTI types\n",
    "    .filter(col(\"lang\") == \"en\")  # English comments only\n",
    "    .filter(col(\"word_count\") > 5)  # Minimum word count\n",
    ")\n",
    "\n",
    "\n",
    "df_comments_typed = df_comments_typed.select(\"type\", \"comment\")\n",
    "print(df_comments_typed.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e379533-a5e8-4621-a9ca-dfade82612a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./lib/python3.12/site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./lib/python3.12/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datasets saved as train_data.pth & test_data.pth\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Drop \"type_indexed\" and \"type_encoded\" columns if they already exist\n",
    "for col_name in [\"type_indexed\", \"type_encoded\"]:\n",
    "    if col_name in df_comments_typed.columns:\n",
    "        df_comments_typed = df_comments_typed.drop(col_name)\n",
    "\n",
    "# Apply StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_indexed\", handleInvalid=\"keep\")\n",
    "df_comments_typed = indexer.fit(df_comments_typed).transform(df_comments_typed)\n",
    "\n",
    "# Apply OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCol=\"type_indexed\", outputCol=\"type_encoded\")\n",
    "df_comments_typed = encoder.fit(df_comments_typed).transform(df_comments_typed)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "df_pandas = df_comments_typed.select(\"comment\", \"type_encoded\").limit(5000).toPandas()\n",
    "df_pandas[\"type_encoded\"] = df_pandas[\"type_encoded\"].apply(lambda x: x.toArray())\n",
    "df_pandas[\"class_label\"] = df_pandas[\"type_encoded\"].apply(lambda x: x.argmax())\n",
    "\n",
    "# Train-Test Split\n",
    "train_df, test_df = train_test_split(df_pandas, test_size=0.1, stratify=df_pandas[\"class_label\"], random_state=42)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize_text(df): \n",
    "    return tokenizer(df[\"comment\"].tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "train_encodings, test_encodings = map(tokenize_text, [train_df, test_df])\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Convert Pandas arrays to proper NumPy float32 before creating Torch tensors\n",
    "train_labels, test_labels = map(\n",
    "    lambda df: torch.tensor(np.stack(df[\"type_encoded\"].values).astype(np.float32)), \n",
    "    [train_df, test_df]\n",
    ")\n",
    "\n",
    "# Save datasets\n",
    "for name, enc, lbl in [(\"train\", train_encodings, train_labels), (\"test\", test_encodings, test_labels)]:\n",
    "    torch.save({\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": lbl}, f\"{name}_data.pth\", _use_new_zipfile_serialization=False)\n",
    "\n",
    "print(\"✅ Datasets saved as train_data.pth & test_data.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422e5bc2-618b-4308-860f-111825a8e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introvert extrivert separation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a96b40-a1fe-48d5-8c3a-df68f5a24ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
